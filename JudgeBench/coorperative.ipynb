{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooperative Interaction on JudgeBench with Position Swapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see SOURCES.txt for further source information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen import ConversableAgent, GroupChat, GroupChatManager\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare a subset of the JudgeBench dataset\n",
    "JudgeBench_Claude = load_dataset(\"ScalerLab/JudgeBench\", split=\"claude\")\n",
    "df = pd.DataFrame(JudgeBench_Claude)\n",
    "df_sampled = df.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "df_final = df_sampled[[\"question\", \"response_A\", \"response_B\", \"label\"]]\n",
    "\n",
    "print(df_final.info())\n",
    "print(df_final.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI configuration from environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment_name = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Define the model configuration for Azure OpenAI API access\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": deployment_name,\n",
    "        \"api_key\": api_key,\n",
    "        \"base_url\": f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": api_version,\n",
    "        \"temperature\": 0,\n",
    "        \"cache_seed\": 42\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config={\"config_list\": config_list}\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the Analyst-Agent\n",
    "agent_1_system_message =f\"\"\"\n",
    "You are an Analyst-Agent in a cooperative reasoning chain.\n",
    "Your task is to make an initial assessment whether Response A or Response B is more factually correct.\n",
    "Give an explanation on your initial assessment using about 200 words.\n",
    "Always begin your output with: \"As an Analyst-Agent in an initial assessment, I think ...\"\n",
    "Always end your output with stating if Response A or B is more factually correct by using a JSON-object with the following format: {{\"response\": A/B}}\n",
    "\"\"\"\n",
    "\n",
    "# Define the system prompt for the Critical-Enhancer-Agent\n",
    "agent_2_system_message =f\"\"\"\n",
    "You are a Critical-Enhancer-Agent in a cooperative reasoning chain.\n",
    "Your task is to carefully review the Analyst-Agent's initial assessment and improve upon it.\n",
    "You may add missing facts, point out inaccuracies or refine the reasoning in a constructive way.\n",
    "Give an explanation for your review and your suggestions for improvement using about 200 words.\n",
    "Always begin your output with: \"Building on the Analyst-Agent's initial assessment, I ...\"\n",
    "Always end your output with stating if Response A or B is more factually correct by using a JSON-object with the following format: {{\"response\": A/B}}\n",
    "\"\"\"\n",
    "\n",
    "# Define the system prompt for the Decision-Agent\n",
    "agent_3_system_message =f\"\"\"\n",
    "You are the final Decision-Agent in a cooperative reasoning chain.\n",
    "Your task is to integrate the insights from the Analyst-Agent and the Critical-Enhancer-Agent into one final judgement.\n",
    "Give an explanation for your final judgement using about 200 words.\n",
    "Always begin your output with: \"As a final Decision-Agent, after carefully reviewing the Analyst-Agents' and Critical-Enhancer-Agents' previous analyses, I think ...\"\n",
    "Always end your output with stating if Response A or B is more factually correct by using a JSON-object with the following format: {{\"response\": A/B}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Initialize the ConversableAgents for system setup\n",
    "agent_1 = ConversableAgent(\n",
    "    name=\"Analyst-Agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=agent_1_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "agent_2 = ConversableAgent(\n",
    "    name=\"Critical-Enhancer-Agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=agent_2_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "agent_3 = ConversableAgent(\n",
    "    name=\"Decision-Agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=agent_3_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Initialize the GroupChat and GroupChatManager for system setup\n",
    "group_chat = GroupChat(\n",
    "    agents=[agent_1, agent_2, agent_3],\n",
    "    messages=[],\n",
    "    max_round=4,\n",
    "    speaker_selection_method=\"round_robin\"\n",
    ")\n",
    "\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate(question, response_A, response_B, label):\n",
    "\n",
    "    message = f\"\"\" \n",
    "    Question: {question}\n",
    "\n",
    "    Response A: {response_A}\n",
    "\n",
    "    Response B: {response_B}\n",
    "    \"\"\"\n",
    "\n",
    "    # Start of code citation [1]\n",
    "    # The following code is adapted from the source above.\n",
    "\n",
    "    chat_results = agent_3.initiate_chat(\n",
    "        group_chat_manager,\n",
    "        message=message,\n",
    "        summary_method=\"last_msg\",\n",
    "    )\n",
    "\n",
    "    # End of code citation [1]\n",
    "\n",
    "    result_str = str(chat_results.chat_history[-1][\"content\"])\n",
    "\n",
    "    pattern = r'\"response\"\\s*:\\s*\"?([AB])\"?'\n",
    "\n",
    "    match = re.search(pattern, result_str)\n",
    "    # If pattern not found, assign \"X\" to indicate invalid response\n",
    "    system_decision = match.group(1) if match else \"X\"\n",
    "\n",
    "    ground_truth = \"A\" if \"A>\" in label else \"B\"\n",
    "\n",
    "    is_correct = system_decision == ground_truth\n",
    "\n",
    "    return {\n",
    "        \"system_decision\": system_decision,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"is_correct\": is_correct\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "num_rows = 100\n",
    "df_final_subset = df_final.head(num_rows)\n",
    "\n",
    "# Evaluate responses in normal position\n",
    "results_1 = []\n",
    "for _, row in tqdm(df_final_subset.iterrows(), total=num_rows, desc=\"Progress\"):\n",
    "    result = evaluate(\n",
    "        question=row[\"question\"],\n",
    "        response_A=row[\"response_A\"],\n",
    "        response_B=row[\"response_B\"],\n",
    "        label=row[\"label\"],\n",
    "    )\n",
    "    results_1.append(result)\n",
    "\n",
    "results_1_df = pd.DataFrame(results_1)\n",
    "results_1_df.to_csv('Results/cooperative_1.csv', index=False)\n",
    "\n",
    "# Evaluate responses in swapped positions\n",
    "results_2 = []\n",
    "for _, row in tqdm(df_final_subset.iterrows(), total=num_rows, desc=\"Progress\"):\n",
    "    result = evaluate(\n",
    "        question=row[\"question\"],\n",
    "        response_A=row[\"response_B\"],  # swap positions\n",
    "        response_B=row[\"response_A\"],  # swap positions\n",
    "        label=row[\"label\"],\n",
    "    )\n",
    "\n",
    "    # Correct system decision and correctness flag after swapping\n",
    "    # If system_decision is \"X\", leave it unchanged\n",
    "    if result[\"system_decision\"] == \"A\":\n",
    "        result[\"system_decision\"] = \"B\"\n",
    "    elif result[\"system_decision\"] == \"B\":\n",
    "        result[\"system_decision\"] = \"A\"\n",
    "\n",
    "    if result[\"is_correct\"] == True:\n",
    "        result[\"is_correct\"] = False\n",
    "    elif result[\"is_correct\"] == False:\n",
    "        result[\"is_correct\"] = True\n",
    "\n",
    "    results_2.append(result)\n",
    "\n",
    "results_2_df = pd.DataFrame(results_2)\n",
    "results_2_df.to_csv('Results/cooperative_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
