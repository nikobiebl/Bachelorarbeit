{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Interaction on JudgeBench with Position Swapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see SOURCES.txt for further source information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen import ConversableAgent\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare a subset of the JudgeBench dataset\n",
    "JudgeBench_Claude = load_dataset(\"ScalerLab/JudgeBench\", split=\"claude\")\n",
    "df = pd.DataFrame(JudgeBench_Claude)\n",
    "df_sampled = df.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "df_final = df_sampled[[\"question\", \"response_A\", \"response_B\", \"label\"]]\n",
    "\n",
    "print(df_final.info())\n",
    "print(df_final.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI configuration from environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment_name = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Define the model configuration for Azure OpenAI API access\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": deployment_name,\n",
    "        \"api_key\": api_key,\n",
    "        \"base_url\": f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": api_version,\n",
    "        \"temperature\": 0,\n",
    "        \"cache_seed\": 42\n",
    "    }\n",
    "]\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the Scientific-Agent\n",
    "agent_1_system_message =f\"\"\"\n",
    "You are a Scientific-Agent with a background in academic research and critical thinking.\n",
    "Your task is to assess which of Response A or Response B is more factually correct. \n",
    "Focus on whether the statements are logically sound, consistent with known facts and science.\n",
    "Give an explanation for your decision using about 200 words.\n",
    "Always begin your output with \"As a Scientific-Agent I think ...\"\n",
    "Always end your output with stating if Response A or B is more factually correct by using a JSON-object with the following format: {{\"response\": A/B}}\n",
    "\"\"\"\n",
    "\n",
    "# Define the system prompt for the Logical-Agent\n",
    "agent_2_system_message =f\"\"\"\n",
    "You are a Logical-Agent specializing in evaluating argument consistency, clarity and coherence.\n",
    "Your task is to analyze Response A and Response B in terms of internal logical structure and factual plausibility.\n",
    "Look for contradictions, fallacies, and misleading reasoning, even if subtle.\n",
    "Give an explanation for your decision using about 200 words.\n",
    "Always begin your output with: \"As a Logical-Agent I think ...\"\n",
    "Always end your output with stating if Response A or B is more factually correct by using a JSON-object with the following format: {{\"response\": A/B}}\n",
    "\"\"\"\n",
    "\n",
    "# Define the system prompt for the Domain-Expert-Agent\n",
    "agent_3_system_message =f\"\"\"\n",
    "You are a Domain-Expert-Agent with applied experience in the relevant field.\n",
    "Your task is to judge which of Response A or Response B reflects real-world knowledge and practical correctness more accurately.\n",
    "Focus on practical validity, typical use cases, and whether the information would hold in applied scenarios.\n",
    "Give an explanation for your decision using about 200 words.\n",
    "Always begin your output with: \"As a Domain-Expert-Agent I think ...\"\n",
    "Always end your output with stating if Response A or B is more factually correct by using a JSON-object with the following format: {{\"response\": A/B}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Initialize the ConversableAgents for system setup\n",
    "initializer = ConversableAgent(\n",
    "    \"initializer\", \n",
    "    llm_config={\"config_list\": config_list},\n",
    "    human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "agent_1 = ConversableAgent(\n",
    "    \"Scientific-Agent\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=agent_1_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "agent_2 = ConversableAgent(\n",
    "    \"Logical-Agent\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=agent_2_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "agent_3 = ConversableAgent(\n",
    "    \"Domain-Expert\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=agent_3_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function with majority voting\n",
    "def evaluate(question, response_A, response_B, label):\n",
    "\n",
    "    message = f\"\"\" \n",
    "    Question: {question}\n",
    "\n",
    "    Response A: {response_A}\n",
    "\n",
    "    Response B: {response_B}\n",
    "    \"\"\"\n",
    "\n",
    "    agents = [agent_1, agent_2, agent_3]\n",
    "    decisions = []\n",
    "\n",
    "    pattern = r'\"response\"\\s*:\\s*\"?([AB])\"?'\n",
    "\n",
    "    ground_truth = \"A\" if \"A>\" in label else \"B\"\n",
    "\n",
    "    for agent in agents:\n",
    "            \n",
    "            # Start of code citation [1]\n",
    "            # The following code is adapted from the source above.\n",
    "\n",
    "            result = initializer.initiate_chat(agent, message=message, max_turns=1)\n",
    "\n",
    "            # End of code citation [1]\n",
    "\n",
    "            result_str = str(result)\n",
    "\n",
    "            match = re.search(pattern, result_str)\n",
    "            # If pattern not found, assign \"X\" to indicate invalid response\n",
    "            decision = match.group(1) if match else \"X\"\n",
    "            decisions.append(decision)\n",
    "\n",
    "    if \"X\" in decisions:\n",
    "        return {\n",
    "            \"system_decision\": \"X\",\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"is_correct\": False\n",
    "        }\n",
    "\n",
    "    system_decision = Counter(decisions).most_common(1)[0][0]\n",
    "    is_correct = system_decision == ground_truth\n",
    "\n",
    "    return {\n",
    "        \"system_decision\": system_decision,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"is_correct\": is_correct\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "num_rows = 100\n",
    "df_final_subset = df_final.head(num_rows)\n",
    "\n",
    "# Evaluate responses in normal position\n",
    "results_1 = []\n",
    "for _, row in tqdm(df_final_subset.iterrows(), total=num_rows, desc=\"Progress\"):\n",
    "    result = evaluate(\n",
    "        question=row[\"question\"],\n",
    "        response_A=row[\"response_A\"],\n",
    "        response_B=row[\"response_B\"],\n",
    "        label=row[\"label\"],\n",
    "    )\n",
    "    results_1.append(result)\n",
    "\n",
    "results_1_df = pd.DataFrame(results_1)\n",
    "results_1_df.to_csv('Results/parallel_1.csv', index=False)\n",
    "\n",
    "# Evaluate responses in swapped positions\n",
    "results_2 = []\n",
    "for _, row in tqdm(df_final_subset.iterrows(), total=num_rows, desc=\"Progress\"):\n",
    "    result = evaluate(\n",
    "        question=row[\"question\"],\n",
    "        response_A=row[\"response_B\"],  # swap positions\n",
    "        response_B=row[\"response_A\"],  # swap positions\n",
    "        label=row[\"label\"],\n",
    "    )\n",
    "\n",
    "    # Correct system decision and correctness flag after swapping\n",
    "    # If system_decision is \"X\", leave it unchanged\n",
    "    if result[\"system_decision\"] == \"A\":\n",
    "        result[\"system_decision\"] = \"B\"\n",
    "    elif result[\"system_decision\"] == \"B\":\n",
    "        result[\"system_decision\"] = \"A\"\n",
    "\n",
    "    if result[\"is_correct\"] == True:\n",
    "        result[\"is_correct\"] = False\n",
    "    elif result[\"is_correct\"] == False:\n",
    "        result[\"is_correct\"] = True\n",
    "\n",
    "    results_2.append(result)\n",
    "\n",
    "results_2_df = pd.DataFrame(results_2)\n",
    "results_2_df.to_csv('Results/parallel_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
