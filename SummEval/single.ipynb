{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7601a43d",
   "metadata": {},
   "source": [
    "# Single Agent on SummEval "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6d3d",
   "metadata": {},
   "source": [
    "Please see SOURCES.txt for further source information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dea290",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81fdd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen import ConversableAgent\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411b20a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf1d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare a subset of the SummEval dataset\n",
    "SummEval_Test = load_dataset(\"mteb/summeval\", split=\"test\")\n",
    "df = pd.DataFrame(SummEval_Test)\n",
    "problematic_indices = [5, 7, 8, 9, 10, 11, 18, 20, 26, 27, 33, 34, 39, 46, 61, 64, 68, 73, 75, 79, 85, 86, 88, 92, 96, 99]\n",
    "df_filtered = df.drop(index=problematic_indices).reset_index(drop=True)\n",
    "df_filtered = df_filtered[[\"text\", \"machine_summaries\", \"relevance\", \"coherence\", \"fluency\", \"consistency\"]]\n",
    "df_exploded = df_filtered.explode([\"machine_summaries\", \"relevance\", \"coherence\", \"fluency\", \"consistency\"]).reset_index(drop=True)\n",
    "df_sampled = df_exploded.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "columns_to_round = [\"relevance\", \"coherence\", \"fluency\", \"consistency\"]\n",
    "df_sampled[columns_to_round] = df_sampled[columns_to_round].astype(float).round().astype(int)\n",
    "df_final = df_sampled\n",
    "\n",
    "print(df_final.info())\n",
    "print(df_final.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f4134",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI configuration from environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment_name = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Define the model configuration for Azure OpenAI API access\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": deployment_name,\n",
    "        \"api_key\": api_key,\n",
    "        \"base_url\": f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": api_version,  \n",
    "        \"temperature\": 0,\n",
    "        \"cache_seed\": 42,\n",
    "    }\n",
    "]\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70b3ab8",
   "metadata": {},
   "source": [
    "## System Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0810d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the Evaluator-Agent\n",
    "agent_system_message = f\"\"\"\n",
    "You are an objective Evaluator-Agent.\n",
    "In this task you will evaluate the quality of a summary written for a news article.\n",
    "To correctly solve this task, follow these steps:\n",
    "\n",
    "    1. Carefully read the news article, be aware of the information it contains.\n",
    "    2. Read the proposed summary.\n",
    "    3. Rate the summary with integer values on a scale from 1 (worst) to 5 (best) by its relevance, consistency, fluency, and coherence.\n",
    "\n",
    "Definitions:\n",
    "    Relevance:\n",
    "        - The rating measures how well the summary captures the key points of the article.\n",
    "        - Consider whether all and only the important aspects are contained in the summary.\n",
    "    Consistency:\n",
    "        - The rating measures whether the facts in the summary are consistent with the facts in the original article.\n",
    "        - Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\n",
    "    Fluency:\n",
    "        - This rating measures the quality of individual sentences, are they well-written and grammatically correct.\n",
    "        - Consider the quality of individual sentences.\n",
    "    Coherence:\n",
    "        - The rating measures the quality of all sentences collectively, to the fit together and sound naturally.\n",
    "        - Consider the quality of the summary as a whole.\n",
    "\n",
    "Give an explanation on your evaluation using about 200 words.\n",
    "Always begin your output with: \"As an objective Evaluator-Agent I think ...\"\n",
    "Always end your output with a JSON object with the following format:{{\"relevance\": score, \"coherence\": score, \"fluency\": score, \"consistency\": score}} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec66c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Initialize the ConversableAgents for system setup\n",
    "initializer = ConversableAgent(\n",
    "    \"initializer\", \n",
    "    llm_config={\"config_list\": config_list},\n",
    "    human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "agent = ConversableAgent(\n",
    "    \"Evaluator-Agent\", \n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=agent_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27ae230",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64de67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate(text, machine_summaries, relevance, coherence, fluency, consistency):\n",
    "    message = f\"\"\" \n",
    "    Article: {text}\n",
    "\n",
    "    Summary: {machine_summaries}\n",
    "    \"\"\"\n",
    "\n",
    "    # Start of code citation [1]\n",
    "    # The following code is adapted from the source above.\n",
    "\n",
    "    result = initializer.initiate_chat(agent, message=message, max_turns=1)\n",
    "\n",
    "    # End of code citation [1]\n",
    "\n",
    "    result_str = str(result)\n",
    "\n",
    "    pattern = r'\"relevance\"\\s*:\\s*(\\d+)'\n",
    "    relevance_score = int(re.search(pattern, result_str).group(1))\n",
    "\n",
    "    pattern = r'\"coherence\"\\s*:\\s*(\\d+)'\n",
    "    coherence_score = int(re.search(pattern, result_str).group(1))\n",
    "\n",
    "    pattern = r'\"fluency\"\\s*:\\s*(\\d+)'\n",
    "    fluency_score = int(re.search(pattern, result_str).group(1))\n",
    "\n",
    "    pattern = r'\"consistency\"\\s*:\\s*(\\d+)'\n",
    "    consistency_score = int(re.search(pattern, result_str).group(1))\n",
    "\n",
    "    relevance_deviation = relevance_score - relevance\n",
    "    coherence_deviation = coherence_score - coherence\n",
    "    fluency_deviation = fluency_score - fluency\n",
    "    consistency_deviation = consistency_score - consistency\n",
    "\n",
    "    return {\n",
    "        \"relevance\": {\n",
    "            \"ground_truth\": relevance,\n",
    "            \"system_decision\": relevance_score,\n",
    "            \"deviation\": relevance_deviation\n",
    "        },\n",
    "        \"coherence\": {\n",
    "            \"ground_truth\": coherence,\n",
    "            \"system_decision\": coherence_score,\n",
    "            \"deviation\": coherence_deviation\n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"ground_truth\": fluency,\n",
    "            \"system_decision\": fluency_score,\n",
    "            \"deviation\": fluency_deviation\n",
    "        },\n",
    "        \"consistency\": {\n",
    "            \"ground_truth\": consistency,\n",
    "            \"system_decision\": consistency_score,\n",
    "            \"deviation\": consistency_deviation\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "num_rows = 100\n",
    "df_subset = df_final.head(num_rows)\n",
    "\n",
    "# Evaluate responses\n",
    "results = []\n",
    "for _, row in tqdm(df_subset.iterrows(), total=num_rows, desc=\"Progress\"):\n",
    "    result = evaluate(\n",
    "        text=row[\"text\"],\n",
    "        machine_summaries=row[\"machine_summaries\"],\n",
    "        relevance=row[\"relevance\"],\n",
    "        coherence=row[\"coherence\"],\n",
    "        fluency=row[\"fluency\"],\n",
    "        consistency=row[\"consistency\"]\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('Results/single.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
