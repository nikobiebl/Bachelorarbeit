{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94b45229",
   "metadata": {},
   "source": [
    "# Cooperative Interaction on SummEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc6040c",
   "metadata": {},
   "source": [
    "Please see SOURCES.txt for further source information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1300fc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen import ConversableAgent, GroupChat, GroupChatManager\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8e433",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare a subset of the SummEval dataset\n",
    "SummEval_Test = load_dataset(\"mteb/summeval\", split=\"test\")\n",
    "df = pd.DataFrame(SummEval_Test)\n",
    "problematic_indices = [5, 7, 8, 9, 10, 11, 18, 20, 26, 27, 33, 34, 39, 46, 61, 64, 68, 73, 75, 79, 85, 86, 88, 92, 96, 99]\n",
    "df_filtered = df.drop(index=problematic_indices).reset_index(drop=True)\n",
    "df_filtered = df_filtered[[\"text\", \"machine_summaries\", \"relevance\", \"coherence\", \"fluency\", \"consistency\"]]\n",
    "df_exploded = df_filtered.explode([\"machine_summaries\", \"relevance\", \"coherence\", \"fluency\", \"consistency\"]).reset_index(drop=True)\n",
    "df_sampled = df_exploded.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "columns_to_round = [\"relevance\", \"coherence\", \"fluency\", \"consistency\"]\n",
    "df_sampled[columns_to_round] = df_sampled[columns_to_round].astype(float).round().astype(int)\n",
    "df_final = df_sampled\n",
    "\n",
    "print(df_final.info())\n",
    "print(df_final.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4125a7",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d505542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI configuration from environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment_name = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Define the model configuration for Azure OpenAI API access\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": deployment_name,\n",
    "        \"api_key\": api_key,\n",
    "        \"base_url\": f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": api_version,\n",
    "        \"temperature\": 0,\n",
    "        \"cache_seed\": 42\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config={\"config_list\": config_list}\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26d1f9",
   "metadata": {},
   "source": [
    "## System Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8a752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the Analyst-Agent\n",
    "agent_1_system_message =f\"\"\"\n",
    "You are an Analyst-Agent in a cooperative reasoning chain.\n",
    "Your task is to make an initial assessment.\n",
    "\n",
    "In this task you will evaluate the quality of a summary written for a news article.\n",
    "To correctly solve this task, follow these steps:\n",
    "\n",
    "    1. Carefully read the news article, be aware of the information it contains.\n",
    "    2. Read the proposed summary.\n",
    "    3. Rate the summary with integer values on a scale from 1 (worst) to 5 (best) by its relevance, consistency, fluency, and coherence.\n",
    "\n",
    "Definitions:\n",
    "    Relevance:\n",
    "        - The rating measures how well the summary captures the key points of the article.\n",
    "        - Consider whether all and only the important aspects are contained in the summary.\n",
    "    Consistency:\n",
    "        - The rating measures whether the facts in the summary are consistent with the facts in the original article.\n",
    "        - Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\n",
    "    Fluency:\n",
    "        - This rating measures the quality of individual sentences, are they well-written and grammatically correct.\n",
    "        - Consider the quality of individual sentences.\n",
    "    Coherence:\n",
    "        - The rating measures the quality of all sentences collectively, to the fit together and sound naturally.\n",
    "        - Consider the quality of the summary as a whole.\n",
    "\n",
    "Give an explanation on your initial assessment using about 200 words.\n",
    "Always begin your output with: \"As an Analyst-Agent in an initial assessment, I think ...\"\n",
    "Always end your output with a JSON object with the following format:{{\"relevance\": score, \"coherence\": score, \"fluency\": score, \"consistency\": score}} \n",
    "\"\"\"\n",
    "\n",
    "# Define the system prompt for the Critical-Enhancer-Agent\n",
    "agent_2_system_message =f\"\"\"\n",
    "You are a Critical-Enhancer-Agent in a cooperative reasoning chain.\n",
    "Your task is to carefully review the Analyst-Agent's initial assessment and improve upon it.\n",
    "You may add missing facts, point out inaccuracies or refine the reasoning in a constructive way.\n",
    "\n",
    "In this task you will evaluate the quality of a summary written for a news article.\n",
    "To correctly solve this task, follow these steps:\n",
    "\n",
    "    1. Carefully read the news article, be aware of the information it contains.\n",
    "    2. Read the proposed summary.\n",
    "    3. Rate the summary with integer values on a scale from 1 (worst) to 5 (best) by its relevance, consistency, fluency, and coherence.\n",
    "\n",
    "Definitions:\n",
    "    Relevance:\n",
    "        - The rating measures how well the summary captures the key points of the article.\n",
    "        - Consider whether all and only the important aspects are contained in the summary.\n",
    "    Consistency:\n",
    "        - The rating measures whether the facts in the summary are consistent with the facts in the original article.\n",
    "        - Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\n",
    "    Fluency:\n",
    "        - This rating measures the quality of individual sentences, are they well-written and grammatically correct.\n",
    "        - Consider the quality of individual sentences.\n",
    "    Coherence:\n",
    "        - The rating measures the quality of all sentences collectively, to the fit together and sound naturally.\n",
    "        - Consider the quality of the summary as a whole.\n",
    "\n",
    "Give an explanation for your review and your suggestions for improvement using about 200 words.\n",
    "Always begin your output with: \"Building on the Analyst-Agent's initial assessment, I ...\"\n",
    "Always end your output with a JSON object with the following format:{{\"relevance\": score, \"coherence\": score, \"fluency\": score, \"consistency\": score}} \n",
    "\"\"\"\n",
    "\n",
    "# Define the system prompt for the Decision-Agent\n",
    "agent_3_system_message =f\"\"\"\n",
    "You are the final Decision-Agent in a cooperative reasoning chain.\n",
    "Your task is to integrate the insights from the Analyst-Agent and the Critical-Enhancer-Agent into one final judgement.\n",
    "\n",
    "In this task you will evaluate the quality of a summary written for a news article.\n",
    "To correctly solve this task, follow these steps:\n",
    "\n",
    "    1. Carefully read the news article, be aware of the information it contains.\n",
    "    2. Read the proposed summary.\n",
    "    3. Rate the summary with integer values on a scale from 1 (worst) to 5 (best) by its relevance, consistency, fluency, and coherence.\n",
    "\n",
    "Definitions:\n",
    "    Relevance:\n",
    "        - The rating measures how well the summary captures the key points of the article.\n",
    "        - Consider whether all and only the important aspects are contained in the summary.\n",
    "    Consistency:\n",
    "        - The rating measures whether the facts in the summary are consistent with the facts in the original article.\n",
    "        - Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\n",
    "    Fluency:\n",
    "        - This rating measures the quality of individual sentences, are they well-written and grammatically correct.\n",
    "        - Consider the quality of individual sentences.\n",
    "    Coherence:\n",
    "        - The rating measures the quality of all sentences collectively, to the fit together and sound naturally.\n",
    "        - Consider the quality of the summary as a whole.\n",
    "\n",
    "Give an explanation for your final judgement using about 200 words.\n",
    "Always begin your output with: \"As a final Decision-Agent, after carefully reviewing the Analyst-Agents' and Critical-Enhancer-Agents' previous analyses, I think ...\"\n",
    "Always end your output with a JSON object with the following format:{{\"relevance\": score, \"coherence\": score, \"fluency\": score, \"consistency\": score}} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Initialize the ConversableAgents for system setup\n",
    "agent_1 = ConversableAgent(\n",
    "    name=\"Analyst-Agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=agent_1_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "agent_2 = ConversableAgent(\n",
    "    name=\"Critical-Enhancer-Agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=agent_2_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "agent_3 = ConversableAgent(\n",
    "    name=\"Decision-Agent\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=agent_3_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Initialize the GroupChat and GroupChatManager for system setup\n",
    "group_chat = GroupChat(\n",
    "    agents=[agent_1, agent_2, agent_3],\n",
    "    messages=[],\n",
    "    max_round=4,\n",
    "    speaker_selection_method=\"round_robin\"\n",
    ")\n",
    "\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea679336",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33066a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate(text, machine_summaries, relevance, coherence, fluency, consistency):\n",
    "    message = f\"\"\" \n",
    "    Article: {text}\n",
    "\n",
    "    Summary: {machine_summaries}\n",
    "    \"\"\"\n",
    "\n",
    "    # Start of code citation [1]\n",
    "    # The following code is adapted from the source above.\n",
    "\n",
    "    chat_results = agent_3.initiate_chat(\n",
    "        group_chat_manager,\n",
    "        message=message,\n",
    "        summary_method=\"last_msg\",\n",
    "    )\n",
    "\n",
    "    # End of code citation [1]\n",
    "\n",
    "    result_str = str(chat_results.chat_history[-1][\"content\"])\n",
    "\n",
    "    pattern = r'\"relevance\"\\s*:\\s*(\\d+)'\n",
    "    relevance_score = int(re.search(pattern, result_str).group(1))\n",
    "\n",
    "    pattern = r'\"coherence\"\\s*:\\s*(\\d+)'\n",
    "    coherence_score = int(re.search(pattern, result_str).group(1))\n",
    "\n",
    "    pattern = r'\"fluency\"\\s*:\\s*(\\d+)'\n",
    "    fluency_score = int(re.search(pattern, result_str).group(1))\n",
    "\n",
    "    pattern = r'\"consistency\"\\s*:\\s*(\\d+)'\n",
    "    consistency_score = int(re.search(pattern, result_str).group(1))\n",
    "\n",
    "    relevance_deviation = relevance_score - relevance\n",
    "    coherence_deviation = coherence_score - coherence\n",
    "    fluency_deviation = fluency_score - fluency\n",
    "    consistency_deviation = consistency_score - consistency\n",
    "\n",
    "    return {\n",
    "        \"relevance\": {\n",
    "            \"ground_truth\": relevance,\n",
    "            \"system_decision\": relevance_score,\n",
    "            \"deviation\": relevance_deviation\n",
    "        },\n",
    "        \"coherence\": {\n",
    "            \"ground_truth\": coherence,\n",
    "            \"system_decision\": coherence_score,\n",
    "            \"deviation\": coherence_deviation\n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"ground_truth\": fluency,\n",
    "            \"system_decision\": fluency_score,\n",
    "            \"deviation\": fluency_deviation\n",
    "        },\n",
    "        \"consistency\": {\n",
    "            \"ground_truth\": consistency,\n",
    "            \"system_decision\": consistency_score,\n",
    "            \"deviation\": consistency_deviation\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c69935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "num_rows = 100\n",
    "df_subset = df_final.head(num_rows)\n",
    "\n",
    "# Evaluate responses\n",
    "results = []\n",
    "for _, row in tqdm(df_subset.iterrows(), total=num_rows, desc=\"Progress\"):\n",
    "    result = evaluate(\n",
    "        text=row[\"text\"],\n",
    "        machine_summaries=row[\"machine_summaries\"],\n",
    "        relevance=row[\"relevance\"],\n",
    "        coherence=row[\"coherence\"],\n",
    "        fluency=row[\"fluency\"],\n",
    "        consistency=row[\"consistency\"]\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('Results/cooperative.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
