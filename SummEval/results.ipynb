{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9676953",
   "metadata": {},
   "source": [
    "# Results Analysis on SummEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbba69d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a588c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from scipy.stats import kendalltau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf50b8",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b885b9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Single – Tabelle pro Dimension ====\n",
      "             Kendall Tau  Ø Abweichung  Ø Ground Truth  Ø System\n",
      "relevance            NaN         1.333           3.667     3.000\n",
      "coherence          0.816         0.667           3.333     2.667\n",
      "fluency            0.000         1.333           4.333     3.000\n",
      "consistency        1.000         0.000           4.000     4.000\n",
      "\n",
      "==== Single – Durchschnittswerte Gesamt ====\n",
      "Ø Kendall Tau (Dimensionen): nan\n",
      "Ø Abweichung (gesamt): 0.833\n",
      "Ø Ground Truth (gesamt): 3.833\n",
      "Ø System (gesamt): 3.167\n",
      "\n",
      "==== Parallel – Tabelle pro Dimension ====\n",
      "             Kendall Tau  Ø Abweichung  Ø Ground Truth  Ø System\n",
      "relevance          0.816         1.000           3.667     2.667\n",
      "coherence          0.816         0.667           3.333     2.667\n",
      "fluency            0.000         1.333           4.333     3.000\n",
      "consistency        1.000         1.000           4.000     3.000\n",
      "\n",
      "==== Parallel – Durchschnittswerte Gesamt ====\n",
      "Ø Kendall Tau (Dimensionen): 0.658\n",
      "Ø Abweichung (gesamt): 1.000\n",
      "Ø Ground Truth (gesamt): 3.833\n",
      "Ø System (gesamt): 2.833\n",
      "\n",
      "==== Cooperative – Tabelle pro Dimension ====\n",
      "             Kendall Tau  Ø Abweichung  Ø Ground Truth  Ø System\n",
      "relevance          0.816         1.000           3.667     2.667\n",
      "coherence          0.333         1.333           3.333     2.000\n",
      "fluency            0.000         1.667           4.333     2.667\n",
      "consistency        1.000         0.333           4.000     3.667\n",
      "\n",
      "==== Cooperative – Durchschnittswerte Gesamt ====\n",
      "Ø Kendall Tau (Dimensionen): 0.537\n",
      "Ø Abweichung (gesamt): 1.083\n",
      "Ø Ground Truth (gesamt): 3.833\n",
      "Ø System (gesamt): 2.750\n",
      "\n",
      "==== Competitive – Tabelle pro Dimension ====\n",
      "             Kendall Tau  Ø Abweichung  Ø Ground Truth  Ø System\n",
      "relevance          0.816         1.333           3.667     2.333\n",
      "coherence          0.816         1.000           3.333     2.333\n",
      "fluency            0.500         2.000           4.333     2.333\n",
      "consistency        0.816         1.333           4.000     2.667\n",
      "\n",
      "==== Competitive – Durchschnittswerte Gesamt ====\n",
      "Ø Kendall Tau (Dimensionen): 0.737\n",
      "Ø Abweichung (gesamt): 1.417\n",
      "Ø Ground Truth (gesamt): 3.833\n",
      "Ø System (gesamt): 2.417\n"
     ]
    }
   ],
   "source": [
    "files = {\n",
    "    \"Single\": \"Results/single.csv\",\n",
    "    \"Parallel\": \"Results/parallel.csv\",\n",
    "    \"Cooperative\": \"Results/cooperative.csv\",\n",
    "    \"Competitive\": \"Results/competitive.csv\"\n",
    "}\n",
    "\n",
    "def parse_and_evaluate(path):\n",
    "    df = pd.read_csv(path)\n",
    "    parsed = {dim: df[dim].apply(ast.literal_eval) for dim in df.columns}\n",
    "    gt = pd.DataFrame({dim: parsed[dim].apply(lambda x: x[\"ground_truth\"]) for dim in parsed})\n",
    "    sys = pd.DataFrame({dim: parsed[dim].apply(lambda x: x[\"system_decision\"]) for dim in parsed})\n",
    "    dev = pd.DataFrame({dim: parsed[dim].apply(lambda x: x[\"deviation\"]) for dim in parsed})\n",
    "    \n",
    "    kendalls = {dim: kendalltau(gt[dim], sys[dim]).correlation for dim in gt.columns}\n",
    "    avg_kendall = sum(kendalls.values()) / len(kendalls)\n",
    "    avg_dev_dim = dev.mean().to_dict()\n",
    "    avg_dev_all = dev.values.flatten().mean()\n",
    "    avg_gt_dim = gt.mean().to_dict()\n",
    "    avg_sys_dim = sys.mean().to_dict()\n",
    "    avg_gt_all = gt.values.flatten().mean()\n",
    "    avg_sys_all = sys.values.flatten().mean()\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        \"Kendall Tau\": kendalls,\n",
    "        \"Ø Abweichung\": avg_dev_dim,\n",
    "        \"Ø Ground Truth\": avg_gt_dim,\n",
    "        \"Ø System\": avg_sys_dim\n",
    "    })\n",
    "\n",
    "    overview = {\n",
    "        \"Ø Kendall Tau (Dimensionen)\": avg_kendall,\n",
    "        \"Ø Abweichung (gesamt)\": avg_dev_all,\n",
    "        \"Ø Ground Truth (gesamt)\": avg_gt_all,\n",
    "        \"Ø System (gesamt)\": avg_sys_all\n",
    "    }\n",
    "\n",
    "    return summary.round(3), overview\n",
    "\n",
    "for name, path in files.items():\n",
    "    summary, overview = parse_and_evaluate(path)\n",
    "    print(f\"\\n==== {name} – Tabelle pro Dimension ====\")\n",
    "    print(summary)\n",
    "    print(f\"\\n==== {name} – Durchschnittswerte Gesamt ====\")\n",
    "    for k, v in overview.items():\n",
    "        print(f\"{k}: {v:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
