{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3490e9",
   "metadata": {},
   "source": [
    "# Parallel Interaction on SummEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2099315",
   "metadata": {},
   "source": [
    "Please see SOURCES.txt for further source information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01400262",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f9af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen import ConversableAgent\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb6d1d",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa47236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare a subset of the SummEval dataset\n",
    "SummEval_Test = load_dataset(\"mteb/summeval\", split=\"test\")\n",
    "df = pd.DataFrame(SummEval_Test)\n",
    "problematic_indices = [5, 7, 8, 9, 10, 11, 18, 20, 26, 27, 33, 34, 39, 46, 61, 64, 68, 73, 75, 79, 85, 86, 88, 92, 96, 99]\n",
    "df_filtered = df.drop(index=problematic_indices).reset_index(drop=True)\n",
    "df_filtered = df_filtered[[\"text\", \"machine_summaries\", \"relevance\", \"coherence\", \"fluency\", \"consistency\"]]\n",
    "df_exploded = df_filtered.explode([\"machine_summaries\", \"relevance\", \"coherence\", \"fluency\", \"consistency\"]).reset_index(drop=True)\n",
    "df_sampled = df_exploded.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "columns_to_round = [\"relevance\", \"coherence\", \"fluency\", \"consistency\"]\n",
    "df_sampled[columns_to_round] = df_sampled[columns_to_round].astype(float).round().astype(int)\n",
    "df_final = df_sampled\n",
    "\n",
    "print(df_final.info())\n",
    "print(df_final.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff1329",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27455074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure OpenAI configuration from environment variables\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "deployment_name = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Define the model configuration for Azure OpenAI API access\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": deployment_name,\n",
    "        \"api_key\": api_key,\n",
    "        \"base_url\": f\"{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version={api_version}\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": api_version,  \n",
    "        \"temperature\": 0,\n",
    "        \"cache_seed\": 42,\n",
    "    }\n",
    "]\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac871f6",
   "metadata": {},
   "source": [
    "## System Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the Factual-Journalist-Agent\n",
    "agent_1_system_message =f\"\"\"\n",
    "You are a Factual-Journalist-Agent.\n",
    "You are a professional journalist focused on verifying factual accuracy, capturing the core message of articles, and ensuring summaries remain true to the source. \n",
    "You value clear, complete, and correct information above all.\n",
    "\n",
    "In this task you will evaluate the quality of a summary written for a news article.\n",
    "To correctly solve this task, follow these steps:\n",
    "\n",
    "    1. Carefully read the news article, be aware of the information it contains.\n",
    "    2. Read the proposed summary.\n",
    "    3. Rate the summary with integer values on a scale from 1 (worst) to 5 (best) by its relevance, consistency, fluency, and coherence.\n",
    "\n",
    "Definitions:\n",
    "    Relevance:\n",
    "        - The rating measures how well the summary captures the key points of the article.\n",
    "        - Consider whether all and only the important aspects are contained in the summary.\n",
    "    Consistency:\n",
    "        - The rating measures whether the facts in the summary are consistent with the facts in the original article.\n",
    "        - Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\n",
    "    Fluency:\n",
    "        - This rating measures the quality of individual sentences, are they well-written and grammatically correct.\n",
    "        - Consider the quality of individual sentences.\n",
    "    Coherence:\n",
    "        - The rating measures the quality of all sentences collectively, to the fit together and sound naturally.\n",
    "        - Consider the quality of the summary as a whole.\n",
    "\n",
    "Give an explanation on your evaluation using about 200 words.\n",
    "Always begin your output with: \"As a Factual-Journalist-Agent I think ...\"\n",
    "Always end your output with a JSON object with the following format:{{\"relevance\": score, \"coherence\": score, \"fluency\": score, \"consistency\": score}} \n",
    "\"\"\"\n",
    "\n",
    "# Define the system prompt for the Language-Stylist-Agent\n",
    "agent_2_system_message =f\"\"\"\n",
    "You are a Language-Stylist-Agent.\n",
    "You are an expert in writing, grammar, and expression, evaluating how summaries communicate effectively through style, clarity, and sentence construction. \n",
    "You care about linguistic precision, elegance, and flow while maintaining content accuracy.\n",
    "\n",
    "In this task you will evaluate the quality of a summary written for a news article.\n",
    "To correctly solve this task, follow these steps:\n",
    "\n",
    "    1. Carefully read the news article, be aware of the information it contains.\n",
    "    2. Read the proposed summary.\n",
    "    3. Rate the summary with integer values on a scale from 1 (worst) to 5 (best) by its relevance, consistency, fluency, and coherence.\n",
    "\n",
    "Definitions:\n",
    "    Relevance:\n",
    "        - The rating measures how well the summary captures the key points of the article.\n",
    "        - Consider whether all and only the important aspects are contained in the summary.\n",
    "    Consistency:\n",
    "        - The rating measures whether the facts in the summary are consistent with the facts in the original article.\n",
    "        - Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\n",
    "    Fluency:\n",
    "        - This rating measures the quality of individual sentences, are they well-written and grammatically correct.\n",
    "        - Consider the quality of individual sentences.\n",
    "    Coherence:\n",
    "        - The rating measures the quality of all sentences collectively, to the fit together and sound naturally.\n",
    "        - Consider the quality of the summary as a whole.\n",
    "\n",
    "Give an explanation on your evaluation using about 200 words.\n",
    "Always begin your output with: \"As a Language-Stylist-Agent I think ...\"\n",
    "Always end your output with a JSON object with the following format:{{\"relevance\": score, \"coherence\": score, \"fluency\": score, \"consistency\": score}} \n",
    "\"\"\"\n",
    "\n",
    "# Define the system prompt for the Cognitive-Agent\n",
    "agent_3_system_message =f\"\"\"\n",
    "You are a Cognitive-Agent.\n",
    "You specialize in how readers process, understand, and retain information. \n",
    "You assess summaries based on logical structure, mental clarity, and how effectively the content can be followed and integrated by readers.\n",
    "\n",
    "In this task you will evaluate the quality of a summary written for a news article.\n",
    "To correctly solve this task, follow these steps:\n",
    "\n",
    "    1. Carefully read the news article, be aware of the information it contains.\n",
    "    2. Read the proposed summary.\n",
    "    3. Rate the summary with integer values on a scale from 1 (worst) to 5 (best) by its relevance, consistency, fluency, and coherence.\n",
    "\n",
    "Definitions:\n",
    "    Relevance:\n",
    "        - The rating measures how well the summary captures the key points of the article.\n",
    "        - Consider whether all and only the important aspects are contained in the summary.\n",
    "    Consistency:\n",
    "        - The rating measures whether the facts in the summary are consistent with the facts in the original article.\n",
    "        - Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\n",
    "    Fluency:\n",
    "        - This rating measures the quality of individual sentences, are they well-written and grammatically correct.\n",
    "        - Consider the quality of individual sentences.\n",
    "    Coherence:\n",
    "        - The rating measures the quality of all sentences collectively, to the fit together and sound naturally.\n",
    "        - Consider the quality of the summary as a whole.\n",
    "\n",
    "Give an explanation on your evaluation using about 200 words.\n",
    "Always begin your output with: \"As a Cognitive-Agent I think ...\"\n",
    "Always end your output with a JSON object with the following format:{{\"relevance\": score, \"coherence\": score, \"fluency\": score, \"consistency\": score}} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb90312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of code citation [1]\n",
    "# The following code is adapted from the source above.\n",
    "\n",
    "# Initialize the ConversableAgents for system setup\n",
    "initializer = ConversableAgent(\n",
    "    \"initializer\", \n",
    "    llm_config={\"config_list\": config_list},\n",
    "    human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "agent_1 = ConversableAgent(\n",
    "    \"Factual-Journalist-Agent\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=agent_1_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "agent_2 = ConversableAgent(\n",
    "    \"Language-Stylist-Agent\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=agent_2_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "agent_3 = ConversableAgent(\n",
    "    \"Cognitive-Agent\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=agent_3_system_message,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# End of code citation [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d13f01",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation function\n",
    "def evaluate(text, machine_summaries, relevance, coherence, fluency, consistency):\n",
    "    message = f\"\"\" \n",
    "    Article: {text}\n",
    "\n",
    "    Summary: {machine_summaries}\n",
    "    \"\"\"\n",
    "\n",
    "    agents = [agent_1, agent_2, agent_3]\n",
    "\n",
    "    scores = {\n",
    "        \"relevance\": [],\n",
    "        \"coherence\": [],\n",
    "        \"fluency\": [],\n",
    "        \"consistency\": []\n",
    "    }\n",
    "\n",
    "    for agent in agents:\n",
    "\n",
    "        # Start of code citation [1]\n",
    "        # The following code is adapted from the source above. \n",
    "\n",
    "        result = initializer.initiate_chat(agent, message=message, max_turns=1)\n",
    "\n",
    "        # End of code citation [1]\n",
    "\n",
    "        result_str = str(result)\n",
    "\n",
    "        relevance_match = re.search(r'\"relevance\"\\s*:\\s*(\\d+)', result_str)\n",
    "        coherence_match = re.search(r'\"coherence\"\\s*:\\s*(\\d+)', result_str)\n",
    "        fluency_match = re.search(r'\"fluency\"\\s*:\\s*(\\d+)', result_str)\n",
    "        consistency_match = re.search(r'\"consistency\"\\s*:\\s*(\\d+)', result_str)\n",
    "\n",
    "        scores[\"relevance\"].append(int(relevance_match.group(1)))\n",
    "        scores[\"coherence\"].append(int(coherence_match.group(1)))\n",
    "        scores[\"fluency\"].append(int(fluency_match.group(1)))\n",
    "        scores[\"consistency\"].append(int(consistency_match.group(1)))\n",
    "\n",
    "\n",
    "    final_scores = {key: round(sum(vals) / len(vals)) for key, vals in scores.items()}\n",
    "\n",
    "    return {\n",
    "        \"relevance\": {\n",
    "            \"ground_truth\": relevance,\n",
    "            \"system_decision\": final_scores[\"relevance\"],\n",
    "            \"deviation\": final_scores[\"relevance\"] - relevance\n",
    "        },\n",
    "        \"coherence\": {\n",
    "            \"ground_truth\": coherence,\n",
    "            \"system_decision\": final_scores[\"coherence\"],\n",
    "            \"deviation\": final_scores[\"coherence\"] - coherence\n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"ground_truth\": fluency,\n",
    "            \"system_decision\": final_scores[\"fluency\"],\n",
    "            \"deviation\": final_scores[\"fluency\"] - fluency\n",
    "        },\n",
    "        \"consistency\": {\n",
    "            \"ground_truth\": consistency,\n",
    "            \"system_decision\": final_scores[\"consistency\"],\n",
    "            \"deviation\": final_scores[\"consistency\"] - consistency\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb1b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare evaluation data\n",
    "num_rows = 100\n",
    "df_subset = df_final.head(num_rows)\n",
    "\n",
    "# Evaluate responses\n",
    "results = []\n",
    "for _, row in tqdm(df_subset.iterrows(), total=num_rows, desc=\"Progress\"):\n",
    "    result = evaluate(\n",
    "        text=row[\"text\"],\n",
    "        machine_summaries=row[\"machine_summaries\"],\n",
    "        relevance=row[\"relevance\"],\n",
    "        coherence=row[\"coherence\"],\n",
    "        fluency=row[\"fluency\"],\n",
    "        consistency=row[\"consistency\"]\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('Results/parallel.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
